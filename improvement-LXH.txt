1.对大数据集跑一下
1.1 Sparse的调用看一下。->直接用full，小一点还能跑，大一点的维度就不行。耿哥你需要看一下这里的接口。
2.写一下sgd -> 即grad的求解时求和做加权。
3.改进一下算法？提升收敛速度。
3.1 如何评价？不同初始点的收敛速度对比，算法的robustness
3.2 如何改进？ gradient：stepsize上有自适应的问题
3.2.1 一个想法，随机挑选初始值，选择梯度最小的进行下降，可以减少迭代次数。（需要考虑进入计算次数）
3.2.2 对于大数据集： 1.写batch training，即obj里只含部分样本，做batch的从小到大合并。
		 2.SGD，对方向进行筛选，和batch思想一致，也是从小到大。
		 3.batch + SGD，二者结合。

12.28
svm的问题
sgd的加速问题
sgd+batch的问题.

sgd调试问题：
当下降到一定程度后，sgd会震荡，前期是更快的。要减小这个震荡，即到一定程度后需要把sgd变为full gradient。
这里可以做自适应吗? 还是根据算法进行调整。
sgd的ratio越小，前期下降越快，后期震荡越早。
sgd的ratio需要和梯度的阶段进行选取。这里的策略如何调试？

简单策略：梯度到达一定阈值则切换为1